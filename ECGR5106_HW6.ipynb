{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anpham1331372/ECGR5106/blob/main/ECGR5106_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfz5Js2YnUYf",
        "outputId": "d1c4e33a-6f88-468b-ab43-d46cb2b17b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ViT Configuration ===\n",
            "{'image_size': 32, 'patch_size': 4, 'embed_dim': 256, 'num_heads': 4, 'num_layers': 4, 'mlp_dim': 512, 'num_classes': 100, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 1/10: Loss=3.9023, Test Accuracy=13.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 2/10: Loss=3.4436, Test Accuracy=19.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 3/10: Loss=3.2495, Test Accuracy=21.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 4/10: Loss=3.1157, Test Accuracy=24.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 5/10: Loss=3.0184, Test Accuracy=25.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 6/10: Loss=2.9389, Test Accuracy=25.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 7/10: Loss=2.8523, Test Accuracy=28.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 8/10: Loss=2.7902, Test Accuracy=29.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 9/10: Loss=2.7091, Test Accuracy=29.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 10/10: Loss=2.6300, Test Accuracy=31.48%\n",
            "\n",
            "=== ResNet-18 Baseline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 1/10: Loss=3.5505, Test Accuracy=23.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 2/10: Loss=2.7920, Test Accuracy=31.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 3/10: Loss=2.3850, Test Accuracy=37.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 4/10: Loss=2.0767, Test Accuracy=40.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 5/10: Loss=1.8171, Test Accuracy=43.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 6/10: Loss=1.5590, Test Accuracy=44.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 7/10: Loss=1.3042, Test Accuracy=44.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 8/10: Loss=1.0535, Test Accuracy=45.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 9/10: Loss=0.8129, Test Accuracy=44.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet] Epoch 10/10: Loss=0.6266, Test Accuracy=43.77%\n",
            "\n",
            "=== Summary Table ===\n",
            "Model              Params         MACs   Accuracy   Time (s)\n",
            "ViT             2,164,068    1,888,868      31.48     367.66\n",
            "ResNet-18      11,227,812   37,072,356      43.77     317.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "#Config 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 4,\n",
        "    \"mlp_dim\": 512,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_macs = info.total_mult_adds\n",
        "    return total_params, total_macs\n",
        "\n",
        "# ====================\n",
        "# Run ViT Experiment\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_macs = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# ResNet-18 Baseline\n",
        "# ====================\n",
        "print(\"\\n=== ResNet-18 Baseline ===\")\n",
        "resnet_model = resnet18(num_classes=config[\"num_classes\"]).to(device)\n",
        "resnet_optimizer = torch.optim.Adam(resnet_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "resnet_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(resnet_model, train_loader, resnet_optimizer, vit_criterion)\n",
        "    resnet_acc = evaluate(resnet_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    resnet_epoch_times.append(epoch_time)\n",
        "    print(f\"[ResNet] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={resnet_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "resnet_total_time = time.time() - start_time\n",
        "\n",
        "resnet_params, resnet_macs = get_model_stats(resnet_model, input_size=(1, 3, 32, 32))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'MACs':>12} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_macs:>12,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n",
        "print(f\"{'ResNet-18':<12} {resnet_params:>12,} {resnet_macs:>12,} {resnet_acc:>10.2f} {resnet_total_time:>15.2f} {sum(resnet_epoch_times)/len(resnet_epoch_times):>20.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Config 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 512,\n",
        "    \"num_heads\": 8,\n",
        "    \"num_layers\": 8,\n",
        "    \"mlp_dim\": 2048,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ],
      "metadata": {
        "id": "SpVrubh_62cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Config 3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 8,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 2,\n",
        "    \"num_layers\": 4,\n",
        "    \"mlp_dim\": 512,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ],
      "metadata": {
        "id": "c35781KEES-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Config 4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 8,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 8,\n",
        "    \"mlp_dim\": 1028,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYiXz-7zFnTe",
        "outputId": "ccd50a06-c348-4add-f106-466dfa0d941c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ViT Configuration ===\n",
            "{'image_size': 32, 'patch_size': 8, 'embed_dim': 256, 'num_heads': 4, 'num_layers': 8, 'mlp_dim': 1028, 'num_classes': 100, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 1/10: Loss=4.1574, Test Accuracy=8.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 2/10: Loss=3.9020, Test Accuracy=10.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 3/10: Loss=3.8231, Test Accuracy=10.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 4/10: Loss=3.8958, Test Accuracy=10.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 5/10: Loss=3.8179, Test Accuracy=10.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 6/10: Loss=3.9201, Test Accuracy=10.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 7/10: Loss=3.8962, Test Accuracy=9.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 8/10: Loss=3.8572, Test Accuracy=11.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 9/10: Loss=3.8770, Test Accuracy=9.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT] Epoch 10/10: Loss=3.9540, Test Accuracy=9.93%\n",
            "\n",
            "=== Summary Table ===\n",
            "Model              Params            MACs           FLOPs   Accuracy   Time (s)\n",
            "ViT             6,414,724       5,054,084      10,108,168       9.93     369.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "image_size = 224  # Swin expects 224x224\n",
        "num_classes = 100\n",
        "\n",
        "# Load image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "# Data transforms for Swin\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pretrained Swin Transformer (Tiny)\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.swin.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Only train classification head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch [{epoch+1}] Training Time: {end_time - start_time:.2f}s | Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nFine-tuning pretrained Swin Transformer (Tiny) on CIFAR-100...\")\n",
        "    train()\n",
        "    print(\"\\nEvaluating fine-tuned model...\")\n",
        "    test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqMjDPjuLz0P",
        "outputId": "17855299-560c-4ec6-8adc-807abfb86527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fine-tuning pretrained Swin Transformer (Tiny) on CIFAR-100...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|| 1563/1563 [04:53<00:00,  5.32it/s, loss=3.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1] Training Time: 293.80s | Loss: 4.0361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/5]: 100%|| 1563/1563 [04:53<00:00,  5.33it/s, loss=2.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2] Training Time: 293.36s | Loss: 3.0449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/5]: 100%|| 1563/1563 [04:53<00:00,  5.32it/s, loss=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3] Training Time: 293.73s | Loss: 2.3686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/5]: 100%|| 1563/1563 [04:52<00:00,  5.35it/s, loss=1.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4] Training Time: 292.22s | Loss: 1.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/5]: 100%|| 1563/1563 [04:53<00:00,  5.33it/s, loss=1.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5] Training Time: 293.30s | Loss: 1.6699\n",
            "\n",
            "Evaluating fine-tuned model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|| 313/313 [00:57<00:00,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 66.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2 small\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SwinForImageClassification, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "image_size = 224\n",
        "num_classes = 100\n",
        "\n",
        "# Load image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-small-patch4-window7-224\")\n",
        "\n",
        "# Data transforms for Swin\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pretrained Swin Transformer (Small)\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-small-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.swin.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Only train classification head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch [{epoch+1}] Training Time: {end_time - start_time:.2f}s | Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nFine-tuning pretrained Swin Transformer (Small) on CIFAR-100...\")\n",
        "    train()\n",
        "    print(\"\\nEvaluating fine-tuned model...\")\n",
        "    test()\n"
      ],
      "metadata": {
        "id": "Bu9UGK5rwaDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMr+D7Oc2uNkiUky4QSMrm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}