{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anpham1331372/ECGR5106/blob/main/ECGR5106_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "onGiYMEUBrbV",
        "outputId": "e43e8d9c-41be-409a-e390-c1d2e3c7835e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchinfo torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfz5Js2YnUYf",
        "outputId": "d1c4e33a-6f88-468b-ab43-d46cb2b17b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ViT Configuration ===\n",
            "{'image_size': 32, 'patch_size': 4, 'embed_dim': 256, 'num_heads': 4, 'num_layers': 4, 'mlp_dim': 512, 'num_classes': 100, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 1/10: Loss=3.9023, Test Accuracy=13.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 2/10: Loss=3.4436, Test Accuracy=19.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 3/10: Loss=3.2495, Test Accuracy=21.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 4/10: Loss=3.1157, Test Accuracy=24.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 5/10: Loss=3.0184, Test Accuracy=25.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 6/10: Loss=2.9389, Test Accuracy=25.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 7/10: Loss=2.8523, Test Accuracy=28.17%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 8/10: Loss=2.7902, Test Accuracy=29.06%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 9/10: Loss=2.7091, Test Accuracy=29.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 10/10: Loss=2.6300, Test Accuracy=31.48%\n",
            "\n",
            "=== ResNet-18 Baseline ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 1/10: Loss=3.5505, Test Accuracy=23.47%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 2/10: Loss=2.7920, Test Accuracy=31.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 3/10: Loss=2.3850, Test Accuracy=37.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 4/10: Loss=2.0767, Test Accuracy=40.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 5/10: Loss=1.8171, Test Accuracy=43.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 6/10: Loss=1.5590, Test Accuracy=44.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 7/10: Loss=1.3042, Test Accuracy=44.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 8/10: Loss=1.0535, Test Accuracy=45.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 9/10: Loss=0.8129, Test Accuracy=44.88%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ResNet] Epoch 10/10: Loss=0.6266, Test Accuracy=43.77%\n",
            "\n",
            "=== Summary Table ===\n",
            "Model              Params         MACs   Accuracy   Time (s)\n",
            "ViT             2,164,068    1,888,868      31.48     367.66\n",
            "ResNet-18      11,227,812   37,072,356      43.77     317.44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "#config 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 4,\n",
        "    \"mlp_dim\": 512,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_macs = info.total_mult_adds\n",
        "    return total_params, total_macs\n",
        "\n",
        "# ====================\n",
        "# Run ViT Experiment\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_macs = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# ResNet-18 Baseline\n",
        "# ====================\n",
        "print(\"\\n=== ResNet-18 Baseline ===\")\n",
        "resnet_model = resnet18(num_classes=config[\"num_classes\"]).to(device)\n",
        "resnet_optimizer = torch.optim.Adam(resnet_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "resnet_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(resnet_model, train_loader, resnet_optimizer, vit_criterion)\n",
        "    resnet_acc = evaluate(resnet_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    resnet_epoch_times.append(epoch_time)\n",
        "    print(f\"[ResNet] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={resnet_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "resnet_total_time = time.time() - start_time\n",
        "\n",
        "resnet_params, resnet_macs = get_model_stats(resnet_model, input_size=(1, 3, 32, 32))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'MACs':>12} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_macs:>12,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n",
        "print(f\"{'ResNet-18':<12} {resnet_params:>12,} {resnet_macs:>12,} {resnet_acc:>10.2f} {resnet_total_time:>15.2f} {sum(resnet_epoch_times)/len(resnet_epoch_times):>20.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpVrubh_62cm"
      },
      "outputs": [],
      "source": [
        "#Config 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 512,\n",
        "    \"num_heads\": 8,\n",
        "    \"num_layers\": 8,\n",
        "    \"mlp_dim\": 2048,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c35781KEES-Z"
      },
      "outputs": [],
      "source": [
        "#Config 3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 8,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 2,\n",
        "    \"num_layers\": 4,\n",
        "    \"mlp_dim\": 512,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYiXz-7zFnTe",
        "outputId": "ccd50a06-c348-4add-f106-466dfa0d941c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ViT Configuration ===\n",
            "{'image_size': 32, 'patch_size': 8, 'embed_dim': 256, 'num_heads': 4, 'num_layers': 8, 'mlp_dim': 1028, 'num_classes': 100, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 1/10: Loss=4.1574, Test Accuracy=8.49%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 2/10: Loss=3.9020, Test Accuracy=10.07%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 3/10: Loss=3.8231, Test Accuracy=10.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 4/10: Loss=3.8958, Test Accuracy=10.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 5/10: Loss=3.8179, Test Accuracy=10.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 6/10: Loss=3.9201, Test Accuracy=10.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 7/10: Loss=3.8962, Test Accuracy=9.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 8/10: Loss=3.8572, Test Accuracy=11.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 9/10: Loss=3.8770, Test Accuracy=9.99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT] Epoch 10/10: Loss=3.9540, Test Accuracy=9.93%\n",
            "\n",
            "=== Summary Table ===\n",
            "Model              Params            MACs           FLOPs   Accuracy   Time (s)\n",
            "ViT             6,414,724       5,054,084      10,108,168       9.93     369.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "#Config 4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================\n",
        "# Configuration\n",
        "# ====================\n",
        "config = {\n",
        "    \"image_size\": 32,\n",
        "    \"patch_size\": 8,\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 8,\n",
        "    \"mlp_dim\": 1028,\n",
        "    \"num_classes\": 100,\n",
        "    \"num_epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ====================\n",
        "# Dataset\n",
        "# ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# ====================\n",
        "# Vision Transformer Components\n",
        "# ====================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(config[\"image_size\"], config[\"patch_size\"], 3, config[\"embed_dim\"])\n",
        "        num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config[\"embed_dim\"]))\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoder(config[\"embed_dim\"], config[\"num_heads\"], config[\"mlp_dim\"], config[\"dropout\"])\n",
        "            for _ in range(config[\"num_layers\"])\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.head = nn.Linear(config[\"embed_dim\"], config[\"num_classes\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ====================\n",
        "# Helpers\n",
        "# ====================\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def get_model_stats(model, input_size):\n",
        "    info = summary(model, input_size=input_size, verbose=0)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_flops = info.total_mult_adds * 2  # MACs to FLOPs\n",
        "    return total_params, total_flops\n",
        "\n",
        "# ====================\n",
        "# Run ViT Training\n",
        "# ====================\n",
        "print(\"=== ViT Configuration ===\")\n",
        "print(config)\n",
        "\n",
        "vit_model = VisionTransformer(config).to(device)\n",
        "vit_criterion = nn.CrossEntropyLoss()\n",
        "vit_optimizer = torch.optim.Adam(vit_model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "vit_epoch_times = []\n",
        "start_time = time.time()\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = train(vit_model, train_loader, vit_optimizer, vit_criterion)\n",
        "    vit_acc = evaluate(vit_model, test_loader)\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = epoch_end - epoch_start\n",
        "    vit_epoch_times.append(epoch_time)\n",
        "    print(f\"[ViT] Epoch {epoch+1}/{config['num_epochs']}: Loss={train_loss:.4f}, Test Accuracy={vit_acc:.2f}%, Time={epoch_time:.2f}s\")\n",
        "vit_total_time = time.time() - start_time\n",
        "\n",
        "vit_params, vit_flops = get_model_stats(vit_model, input_size=(1, 3, config[\"image_size\"], config[\"image_size\"]))\n",
        "\n",
        "# ====================\n",
        "# Final Summary\n",
        "# ====================\n",
        "print(\"\\n=== ViT Summary Table ===\")\n",
        "print(f\"{'Model':<12} {'Params':>12} {'FLOPs':>15} {'Accuracy':>10} {'Total Time (s)':>15} {'Avg Epoch Time (s)':>20}\")\n",
        "print(f\"{'ViT':<12} {vit_params:>12,} {vit_flops:>15,} {vit_acc:>10.2f} {vit_total_time:>15.2f} {sum(vit_epoch_times)/len(vit_epoch_times):>20.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ====================\n",
        "# Swin Transformer Core Modules\n",
        "# ====================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout2(x)\n",
        "        return x\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class SwinBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, num_heads, window_size=7)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=112, patch_size=8, in_channels=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # [B, C, H/patch, W/patch]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, N, C]\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class SwinTransformerScratch(nn.Module):\n",
        "    def __init__(self, img_size=112, patch_size=8, in_chans=3, num_classes=100, embed_dim=96, depth=4, num_heads=3):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            SwinBlock(embed_dim, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        x = x.mean(dim=1)  # global average pooling\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "# ====================\n",
        "# Training Setup\n",
        "# ====================\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "image_size = 112\n",
        "\n",
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = SwinTransformerScratch(img_size=112, patch_size=8).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "print(\"Training Swin Transformer from scratch...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch [{epoch+1}] - Loss: {running_loss/len(train_loader):.4f} | Time: {end_time - start_time:.2f}s\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nTest Accuracy of Swin Transformer (Scratch): {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "SVzWlgtnuKxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqMjDPjuLz0P",
        "outputId": "17855299-560c-4ec6-8adc-807abfb86527"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fine-tuning pretrained Swin Transformer (Tiny) on CIFAR-100...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]: 100%|██████████| 1563/1563 [04:53<00:00,  5.32it/s, loss=3.54]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1] Training Time: 293.80s | Loss: 4.0361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/5]: 100%|██████████| 1563/1563 [04:53<00:00,  5.33it/s, loss=2.17]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2] Training Time: 293.36s | Loss: 3.0449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/5]: 100%|██████████| 1563/1563 [04:53<00:00,  5.32it/s, loss=1.22]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3] Training Time: 293.73s | Loss: 2.3686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/5]: 100%|██████████| 1563/1563 [04:52<00:00,  5.35it/s, loss=1.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4] Training Time: 292.22s | Loss: 1.9390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [5/5]: 100%|██████████| 1563/1563 [04:53<00:00,  5.33it/s, loss=1.66]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5] Training Time: 293.30s | Loss: 1.6699\n",
            "\n",
            "Evaluating fine-tuned model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 313/313 [00:57<00:00,  5.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 66.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#problem 2 - tiny\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "image_size = 224  # Swin expects 224x224\n",
        "num_classes = 100\n",
        "\n",
        "# Load image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "# Data transforms for Swin\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pretrained Swin Transformer (Tiny)\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.swin.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Only train classification head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch [{epoch+1}] Training Time: {end_time - start_time:.2f}s | Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nFine-tuning pretrained Swin Transformer (Tiny) on CIFAR-100...\")\n",
        "    train()\n",
        "    print(\"\\nEvaluating fine-tuned model...\")\n",
        "    test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu9UGK5rwaDq"
      },
      "outputs": [],
      "source": [
        "#problem 2 small\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SwinForImageClassification, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "image_size = 224\n",
        "num_classes = 100\n",
        "\n",
        "# Load image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-small-patch4-window7-224\")\n",
        "\n",
        "# Data transforms for Swin\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pretrained Swin Transformer (Small)\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-small-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.swin.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Only train classification head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch [{epoch+1}] Training Time: {end_time - start_time:.2f}s | Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nFine-tuning pretrained Swin Transformer (Small) on CIFAR-100...\")\n",
        "    train()\n",
        "    print(\"\\nEvaluating fine-tuned model...\")\n",
        "    test()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNIuJqbcD1Kjhb4Ejno32Pb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}